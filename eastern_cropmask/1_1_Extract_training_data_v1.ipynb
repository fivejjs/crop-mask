{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=True, cloud_defaults=True)\n",
    "\n",
    "#import deafrica specific functions\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_plotting import map_shapefile\n",
    "from deafrica_classificationtools import collect_training_data \n",
    "\n",
    "#import the custom feature layer functions\n",
    "from feature_layer_functions import gm_mads_two_seasons_training\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/Eastern_training_data_20201215.geojson' \n",
    "field = 'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37617</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>25</li>\n",
       "  <li><b>Cores: </b>25</li>\n",
       "  <li><b>Memory: </b>10.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37617' processes=25 threads=25, memory=10.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(threads_per_worker=1, n_workers=25, memory_limit=400e6)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data shapefile\n",
    "gdf = gpd.read_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49666 -3.30737, 32.49693 -3.30716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49314 -3.30836, 32.49382 -3.30847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.49962 -3.31316, 32.50028 -3.31338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.51721 -3.10441, 32.51716 -3.10465...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((32.38058 -2.69827, 32.38091 -2.69820...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                           geometry\n",
       "0      1  POLYGON ((32.49666 -3.30737, 32.49693 -3.30716...\n",
       "1      1  POLYGON ((32.49314 -3.30836, 32.49382 -3.30847...\n",
       "2      1  POLYGON ((32.49962 -3.31316, 32.50028 -3.31338...\n",
       "3      1  POLYGON ((32.51721 -3.10441, 32.51716 -3.10465...\n",
       "4      1  POLYGON ((32.38058 -2.69827, 32.38091 -2.69820..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up a few extra inputs for `collect_training_data` and the datacube.  See the function docs [here](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/03b7b41d5f6526ff3f33618f7a0b48c0d10a155f/Scripts/deafrica_classificationtools.py#L650) for more information on these parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': ('2019-01', '2019-12'),\n",
       " 'measurements': ['red',\n",
       "  'blue',\n",
       "  'green',\n",
       "  'nir',\n",
       "  'swir_1',\n",
       "  'swir_2',\n",
       "  'red_edge_1',\n",
       "  'red_edge_2',\n",
       "  'red_edge_3'],\n",
       " 'resolution': (-20, 20),\n",
       " 'output_crs': 'epsg:6933',\n",
       " 'group_by': 'solar_day'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "zonal_stats = 'median'\n",
    "return_coords = True\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "products = ['s2_l2a']\n",
    "time = ('2019-01', '2019-12')\n",
    "measurements = [\n",
    "    'red', 'blue', 'green', 'nir', 'swir_1', 'swir_2', 'red_edge_1',\n",
    "    'red_edge_2', 'red_edge_3'\n",
    "]\n",
    "resolution = (-20, 20)\n",
    "output_crs = 'epsg:6933'\n",
    "\n",
    "#generate a new datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'group_by' : 'solar_day',\n",
    "}\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.14 ms, sys: 0 ns, total: 7.14 ms\n",
      "Wall time: 7.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, row in enumerate(gdf.itertuples()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"31.65110668617095 -4.243635504601939 0.00044773204944448253 0.00034754945408543847\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,-8.486923459749793)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"8.954640988889651e-06\" opacity=\"0.6\" d=\"M 31.65112326883945,-4.243618921933441 L 31.6515378355519,-4.243618921933441 L 31.6515378355519,-4.243304537816352 L 31.65112326883945,-4.243304537816352 L 31.65112326883945,-4.243618921933441 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7f03882da358>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 244 ms, sys: 4.88 ms, total: 249 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, row in gdf.iterrows():\n",
    "    pass\n",
    "# CPU times: user 243 ms, sys: 4.01 ms, total: 247 ms\n",
    "# Wall time: 247 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_func = gm_mads_two_seasons_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# column_names = []\n",
    "\n",
    "# # loop through polys and extract training data\n",
    "# _get_training_data_for_shp(gdf, index, row, results, column_names,\n",
    "#                                products, dc_query, return_coords=True,\n",
    "#                                custom_func, # gm_mads_two_seasons_training\n",
    "#                               field, # class label\n",
    "#                                calc_indices, # None\n",
    "#                                reduce_func, drop, zonal_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_s3_access(aws_unsigned=True, cloud_defaults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.utils import geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query based on polygon (convert to WGS84)\n",
    "geom = geometry.Geometry(gdf.geometry.values[index],\n",
    "                         geometry.CRS('epsg:4326'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shapely.geometry.polygon.Polygon, shapely.geometry.polygon.Polygon)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gdf.geometry.values[index]), type(row.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datacube.utils.geometry._base.Geometry"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(geom)\n",
    "q = {\"geopolygon\": geom}\n",
    "\n",
    "# merge polygon query with user supplied query params\n",
    "query.update(q)\n",
    "\n",
    "query['dask_chunk'] = {'time': -1, 'x': 10000, 'y': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': ('2019-01', '2019-12'),\n",
       " 'measurements': ['red',\n",
       "  'blue',\n",
       "  'green',\n",
       "  'nir',\n",
       "  'swir_1',\n",
       "  'swir_2',\n",
       "  'red_edge_1',\n",
       "  'red_edge_2',\n",
       "  'red_edge_3'],\n",
       " 'resolution': (-20, 20),\n",
       " 'output_crs': 'epsg:6933',\n",
       " 'group_by': 'solar_day',\n",
       " 'geopolygon': Geometry(POLYGON ((31.65112326883945 -4.243618921933441, 31.6515378355519 -4.243618921933441, 31.6515378355519 -4.243304537816352, 31.65112326883945 -4.243304537816352, 31.65112326883945 -4.243618921933441)), epsg:4326),\n",
       " 'dask_chunk': {'time': -1, 'x': 10000, 'y': 1000}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function feature_layer_functions.gm_mads_two_seasons_training(ds)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deafrica_datahandling import mostcommon_crs, load_ard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_ard(dc=dc, products=products, **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'nir',\n",
       " 'swir_1',\n",
       " 'swir_2',\n",
       " 'red_edge_1',\n",
       " 'red_edge_2',\n",
       " 'red_edge_3']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_type = 's2'\n",
    "fmask_band = 'SCL'\n",
    "\n",
    "measurements.append(fmask_band)\n",
    "# \n",
    "data_bands = [band for band in measurements if band not in (fmask_band)]\n",
    "mask_bands = [band for band in measurements if band not in data_bands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['red',\n",
       "  'blue',\n",
       "  'green',\n",
       "  'nir',\n",
       "  'swir_1',\n",
       "  'swir_2',\n",
       "  'red_edge_1',\n",
       "  'red_edge_2',\n",
       "  'red_edge_3'],\n",
       " ['SCL'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bands, mask_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'time': ('2019-01', '2019-06'),\n",
    " 'measurements': ['red',\n",
    "  'blue',\n",
    "  'green',\n",
    "  'nir',\n",
    "  'swir_1',\n",
    "  'swir_2',\n",
    "  'red_edge_1',\n",
    "  'red_edge_2',\n",
    "  'red_edge_3'],\n",
    " 'resolution': (-20, 20),\n",
    " 'output_crs': 'epsg:6933',\n",
    " 'group_by': 'solar_day'}\n",
    " #'geopolygon': Geometry(POLYGON ((31.65112326883945 -4.243618921933441, 31.6515378355519 -4.243618921933441, 31.6515378355519 -4.243304537816352, 31.65112326883945 -4.243304537816352, 31.65112326883945 -4.243618921933441)), epsg:4326),\n",
    " #'dask_chunk': {'time': -1, 'x': 10000, 'y': 1000}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_datasets too slow, one thread ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_list = []\n",
    "# datasets = dc.find_datasets(product=products, **query)\n",
    "# dataset_list.extend(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243016"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset <id=57835116-d60d-505b-8f42-69d21fb1d37c product=s2_l2a location=s3://sentinel-cogs/sentinel-s2-l2a-cogs/2019/S2B_29NLH_20190111_0_L2A/S2B_29NLH_20190111_0_L2A.json>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('intermediate/dataset_list.pkl', 'rb') as fh:\n",
    "    dataset_list = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 243016)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_list), len(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_categories_s2=['vegetation','snow or ice',\n",
    "                               'water','bare soils',\n",
    "                               'unclassified', 'dark area pixels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='training_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading too slow, one thread ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask_chunks = {'time': -1, 'x': 10000, 'y': 10000}\n",
    "\n",
    "ds = dc.load(datasets=dataset_list,\n",
    "#              dask_chunks=dask_chunks, \n",
    "             **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Out',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'quit']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_zarr('intermediate/data_retrieved.zarr', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentinel 2                     \n",
    "if product_type == 's2':\n",
    "    # product_type is s2\n",
    "    # currently broken for mask band values >=8\n",
    "    # pq_mask = odc.algo.fmask_to_bool(ds[fmask_band],\n",
    "    #                             categories=pq_categories_s2)\n",
    "    flags_s2 = dc.list_measurements().loc[products[0]].loc[fmask_band]['flags_definition']['qa']['values']\n",
    "    pq_mask = ds[fmask_band].isin([int(k) for k, v in flags_s2.items() if v in pq_categories_s2])\n",
    "\n",
    "\n",
    "###############\n",
    "# Apply masks #\n",
    "###############\n",
    "\n",
    "# Generate good quality data mask\n",
    "mask = None\n",
    "if mask_pixel_quality:\n",
    "    print('Applying pixel quality/cloud mask')\n",
    "    mask = pq_mask\n",
    "\n",
    "ds_data = ds[data_bands]\n",
    "ds_masks = ds[mask_bands]\n",
    "\n",
    "# Mask data if either of the above masks were generated\n",
    "if mask is not None:\n",
    "    ds_data = odc.algo.keep_good_only(ds_data, where=mask)\n",
    "\n",
    "# Automatically set dtype to either native or float32 depending\n",
    "# on whether masking was requested\n",
    "if dtype == 'auto':\n",
    "    dtype = 'native' if mask is None else 'float32'\n",
    "\n",
    "# Set nodata values using odc.algo tools to reduce peak memory\n",
    "# use when converting data dtype    \n",
    "if dtype != 'native':\n",
    "    ds_data = odc.algo.to_float(ds_data, dtype=dtype)\n",
    "\n",
    "\n",
    "attrs = ds.attrs\n",
    "ds = xr.merge([ds_data, ds_masks])\n",
    "ds.attrs.update(attrs)\n",
    "\n",
    "###############\n",
    "# Return data #\n",
    "###############\n",
    "\n",
    "# Drop bands not originally requested by user\n",
    "requested_measurements = measurements\n",
    "if requested_measurements:\n",
    "    ds = ds[requested_measurements]\n",
    "\n",
    "\n",
    "# If user supplied dask_chunks, return data as a dask array without\n",
    "# actually loading it in\n",
    "if dask_chunks is not None:\n",
    "    # skipped\n",
    "    print(f'Returning {len(ds.time)} time steps as a dask array')\n",
    "    return ds\n",
    "else:\n",
    "    print(f'Loading {len(ds.time)} time steps')\n",
    "    # TODO: only use lazay, remove compute() below\n",
    "    return ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining of _get_training_data_for_shp\n",
    "# create polygon mask\n",
    "with HiddenPrints():\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], ds)\n",
    "\n",
    "# Use custom function for training data if it exists\n",
    "if custom_func is not None:\n",
    "    with HiddenPrints():\n",
    "        data = custom_func(ds)\n",
    "        data = data.where(mask)\n",
    "\n",
    "else:\n",
    "    # mask dataset\n",
    "    ds = ds.where(mask)\n",
    "    # first check enough variables are set to run functions\n",
    "    if (len(ds.time.values) > 1) and (reduce_func == None):\n",
    "        raise Exception(\n",
    "            \"You're dataset has \" + str(len(ds.time.values)) +\n",
    "            \" time-steps, please provide a time reduction function,\" +\n",
    "            \" e.g. reduce_func='mean'\")\n",
    "\n",
    "    if calc_indices is not None:\n",
    "        # determine which collection is being loaded\n",
    "        if 'level2' in products[0]:\n",
    "            collection = 'c2'\n",
    "        elif 'gm' in products[0]:\n",
    "            collection = 'c2'\n",
    "        elif 'sr' in products[0]:\n",
    "            collection = 'c1'\n",
    "        elif 's2' in products[0]:\n",
    "            collection = 's2'\n",
    "\n",
    "        if len(ds.time.values) > 1:\n",
    "\n",
    "            if reduce_func in ['mean', 'median', 'std', 'max', 'min']:\n",
    "                with HiddenPrints():\n",
    "                    data = calculate_indices(ds,\n",
    "                                             index=calc_indices,\n",
    "                                             drop=drop,\n",
    "                                             collection=collection)\n",
    "                    # getattr is equivalent to calling data.reduce_func\n",
    "                    method_to_call = getattr(data, reduce_func)\n",
    "                    data = method_to_call(dim='time')\n",
    "\n",
    "            elif reduce_func == 'geomedian':\n",
    "                data = GeoMedian().compute(ds)\n",
    "                with HiddenPrints():\n",
    "                    data = calculate_indices(data,\n",
    "                                             index=calc_indices,\n",
    "                                             drop=drop,\n",
    "                                             collection=collection)\n",
    "\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    reduce_func + \" is not one of the supported\" +\n",
    "                    \" reduce functions ('mean','median','std','max','min', 'geomedian')\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            with HiddenPrints():\n",
    "                data = calculate_indices(ds,\n",
    "                                         index=calc_indices,\n",
    "                                         drop=drop,\n",
    "                                         collection=collection)\n",
    "\n",
    "    # when band indices are not required, reduce the\n",
    "    # dataset to a 2d array through means or (geo)medians\n",
    "    if calc_indices is None:\n",
    "\n",
    "        if len(ds.time.values) > 1:\n",
    "\n",
    "            if reduce_func == 'geomedian':\n",
    "                data = GeoMedian().compute(ds)\n",
    "\n",
    "            elif reduce_func in ['mean', 'median', 'std', 'max', 'min']:\n",
    "                method_to_call = getattr(ds, reduce_func)\n",
    "                data = method_to_call('time')\n",
    "        else:\n",
    "            data = ds.squeeze()\n",
    "\n",
    "if return_coords == True:\n",
    "    # turn coords into a variable in the ds\n",
    "    data['x_coord'] = ds.x + 0 * ds.y\n",
    "    data['y_coord'] = ds.y + 0 * ds.x\n",
    "\n",
    "if zonal_stats is None:\n",
    "    # If no zonal stats were requested then extract all pixel values\n",
    "    flat_train = sklearn_flatten(data)\n",
    "    flat_val = np.repeat(row[field], flat_train.shape[0])\n",
    "    stacked = np.hstack((np.expand_dims(flat_val, axis=1), flat_train))\n",
    "\n",
    "elif zonal_stats in ['mean', 'median', 'std', 'max', 'min']:\n",
    "    method_to_call = getattr(data, zonal_stats)\n",
    "    flat_train = method_to_call()\n",
    "    flat_train = flat_train.to_array()\n",
    "    stacked = np.hstack((row[field], flat_train))\n",
    "\n",
    "else:\n",
    "    raise Exception(zonal_stats + \" is not one of the supported\" +\n",
    "                    \" reduce functions ('mean','median','std','max','min')\")\n",
    "\n",
    "#return unique-id so we can index if dc.load fails silently\n",
    "_id = gdf.iloc[index]['id']\n",
    "\n",
    "# Append training data and labels to list\n",
    "out_arrs.append(np.append(stacked, _id))\n",
    "out_vars.append([field] + list(data.data_vars) + ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class', 'red_S1', 'blue_S1', 'green_S1', 'nir_S1', 'swir_1_S1', 'swir_2_S1', 'red_edge_1_S1', 'red_edge_2_S1', 'red_edge_3_S1', 'edev_S1', 'sdev_S1', 'bcdev_S1', 'NDVI_S1', 'LAI_S1', 'MNDWI_S1', 'rain_S1', 'red_S2', 'blue_S2', 'green_S2', 'nir_S2', 'swir_1_S2', 'swir_2_S2', 'red_edge_1_S2', 'red_edge_2_S2', 'red_edge_3_S2', 'edev_S2', 'sdev_S2', 'bcdev_S2', 'NDVI_S2', 'LAI_S2', 'MNDWI_S2', 'rain_S2', 'slope', 'x_coord', 'y_coord']\n",
      "\n",
      "[[       1.          0.13        0.08 ...        3.    3137450.\n",
      "   -395860.  ]\n",
      " [       1.          0.09        0.06 ...        3.    3135790.\n",
      "   -422500.  ]\n",
      " [       1.          0.09        0.06 ...        2.95  3135520.\n",
      "   -421710.  ]\n",
      " ...\n",
      " [       1.          0.05        0.05 ...        5.56  3660080.\n",
      "  -1321340.  ]\n",
      " [       1.          0.09        0.05 ...        6.01  3945510.\n",
      "    926100.  ]\n",
      " [       1.          0.12        0.07 ...       11.84  3316340.\n",
      "     88750.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(column_names)\n",
    "print('')\n",
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate the coordinates\n",
    "\n",
    "By setting `return_coords=True` in the `collect_training_data` function, our training data now has two extra columns called `x_coord` and `y_coord`.  We need to seperate these from our training dataset as they will not be used to train the machine learning model. Instead, these variables will be used to help conduct Spatial K-fold Cross validation (SKVC) in the notebook `3_Train_fit_evaluate_classifier`.  For more information on why this is important, see this [article](https://www.tandfonline.com/doi/abs/10.1080/13658816.2017.1346255?journalCode=tgis20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_filename = \"results/training_data/training_data_coordinates_20201217.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_variables = ['x_coord', 'y_coord']\n",
    "model_col_indices = [column_names.index(var_name) for var_name in coord_variables]\n",
    "\n",
    "np.savetxt(coordinates_filename, model_input[:, model_col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Once we've collected all the training data we require, we can write the data to disk. This will allow us to import the data in the next step(s) of the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = \"results/training_data/gm_mads_two_seasons_training_data_20201217.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all columns except the x-y coords\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[0:-2]]\n",
    "#Export files to disk\n",
    "np.savetxt(output_file, model_input[:, model_col_indices], header=\" \".join(column_names[0:-2]), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "To continue working through the notebooks in this `Eastern Africa Cropland Mask` workflow, go to the next notebook `2_Inspect_training_data.ipynb`.\n",
    "\n",
    "1. **Extracting_training_data (this notebook)** \n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Last modified:** Dec 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
